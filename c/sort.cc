//------------------------------------------------------------------------------
// This Source Code Form is subject to the terms of the Mozilla Public
// License, v. 2.0. If a copy of the MPL was not distributed with this
// file, You can obtain one at http://mozilla.org/MPL/2.0/.
//
// Â© H2O.ai 2018
//------------------------------------------------------------------------------
// Sorting/ordering functions.
//
// We use stable parallel MSD Radix sort algorithm, with fallback to Insertion
// sort at small `n`s.
//
// All functions defined here treat the input arrays as immutable -- i.e. no
// in-place sorting. Instead, each function creates and returns an "ordering"
// vector. The ordering `o` of an array `x` is such a sequence of integers that
// array
//     [x[o[i]] for i in range(n)]
// is sorted in ascending order. The sorting is stable, and will gather all NA
// values in `x` (if any) at the beginning of the sorted list.
//
// See also:
//      https://en.wikipedia.org/wiki/Radix_sort
//      https://en.wikipedia.org/wiki/Insertion_sort
//      http://stereopsis.com/radix.html
//      /microbench/sort
//
// Based on Radix sort implementation in (R)data.table:
//      https://github.com/Rdatatable/data.table/src/forder.c
//      https://github.com/Rdatatable/data.table/src/fsort.c
//
//
// Algorithm outline
// =================
//
// 1. Data preparation step.
//
//    Before data can be sorted, it has to be "prepared". The purpose of this
//    transformation is 1) to transform all types into unsigned integers which
//    can be sorted via radix-sort algorithm; 2) to collect values into a
//    contiguous buffer if they are originally shuffled by a RowIndex; 3) to
//    reduce the range of possible values and thus the amount of sorting that
//    has to be done; 4) to apply parameters `na_position` and `ascending`.
//
//    The result of this step is an array `x` of type `uintN_t[n]`, where N
//    depends on the input column. Thus, during this step we try to map the
//    original data into the unsigned integers, making sure that such
//    transformation preserves the order of the original elements.
//
//    For integers, this transformation is merely subtracting the min value and
//    relocating NA to the beginning of the range. For floats, we perform a
//    more complicated bit twiddling (see description below). For strings the
//    data cannot be prepared "fully", so we merely map the first byte into `x`.
//
// 2. Build histogram.
//
//    The core of the MSB Radix Sort algorithm is that we look at first
//    `nradixbits` most significant bits of each number (these bits are called
//    the "radix"), and sort the input according to those prefixes.
//
//    In this step we compute the frequency table (or histogram) of the radixes
//    of each element in the input array `x`. The histogram is computed
//    separately for each chunk of the input used during parallel processing.
//    This step produces a cumulative `histogram` of the data, where for each
//    possible radix prefix and for each input chunk, the value in the histogram
//    is the location within the final sorted array where all values in `x` with
//    specific radix/chunk should go.
//
// 3. Reorder data.
//
//    Shuffle the input data according to the ordering implied by the histogram
//    computed in the previous step. Also, update values of `x` if further
//    sorting is needed. Here "updating" involves removing the initial radix
//    prefix (which was already sorted), sometimes also reducing the elemsize
//    of `x`. For strings we copy the next character of each string into the
//    buffer `x`.
//
// 4. Recursion.
//
//    At this point the data is already stably-sorted according to its most
//    significant `nradixbits` (or for strings, by their first characters), and
//    the values in `x` are properly transformed for further sorting. Also, the
//    `histogram` matrix from step 2 carries information about where each
//    pre-sorted group is located. All we need to do is to sort values within
//    each of those groups, and the job will be done.
//
//    Each sub-group that needs to be sorted will have a different size, and we
//    process each such group taking into account its size: for small groups
//    use insertion-sort and process all those groups in-parallel; larger groups
//    can be sorted in-parallel too, using radix-sort; largest groups are sorted
//    one-by-one, each one one with parallel radix-sort algorithm.
//
//
// Grouping
// ========
//
// Sorting naturally produces information about groups with same values, and
// this information is even necessary in order to sort by multiple columns.
//
// For small arrays that are sorted with insert-sort, group sizes can be
// computed simply by comparing adjacent values.
//
// For radix-sort, if there is no recursion step then group information can be
// derived from the histogram. If radix-sort recurses, then large subgroups
// (which are processed one-by-one) can simply push group sizes into the common
// group stack. However when smaller subgroups are processed in parallel, then
// each such processor needs to write its grouping information into a separate
// buffer, which are then merged into the main group stack.
//
//
//------------------------------------------------------------------------------
#include "sort.h"
#include <algorithm>  // std::min
#include <cstdlib>    // std::abs
#include <cstring>    // std::memset, std::memcpy
#include <vector>     // std::vector
#include "column.h"
#include "datatable.h"
#include "options.h"
#include "rowindex.h"
#include "types.h"
#include "utils.h"
#include "utils/array.h"
#include "utils/assert.h"
#include "utils/omp.h"


/**
 * Data structure that holds all the variables needed to perform radix sort.
 * This object is passed around between the functions that represent different
 * steps in the sorting process, each function reading/writing some of these
 * parameters. The documentation for each function will describe which specific
 * parameters it writes.
 *
 * x
 *   The main data array, depending on `elemsize` has one of the following
 *   types: `uint8_t*`, `uint16_t*`, `uint32_t*` or `uint64_t*`. The array has
 *   `n` elements. This array serves as a "sorting key" -- the final goal is
 *   to produce the ordering of values in `x`.
 *   The elements in `x` are always unsigned, and will be sorted accordingly.
 *   In particular, the data must usually be transformed in order to ensure
 *   that it sorts correctly (this is done in `prepare_input` step).
 *
 * o
 *   Current ordering (row indices) of elements in `x`. This is an array of size
 *   `n` (same as `x`). If present, then this array will be sorted according
 *   to the values `x`. If nullptr, then it will be treated as if `o[j] == j`.
 *
 * n
 *   Number of elements in arrays `x` and `o`.
 *
 * elemsize
 *   Size in bytes of each element in `x`, one of: 1, 2, 4, or 8.
 *
 * strdata, stroffs
 *   For string columns only, these are pointers `col->strdata()` and
 *   `col->offsets()` respectively.
 *
 * strtype
 *   0 when sorting non-strings, 1 if sorting str32, 2 if sorting str64
 *
 * strstart
 *   For string columns only, this is the position within the string that is
 *   currently being tested. More specifically, at the beginning of a
 *   radix_sort() call, `strstart` will indicate the offset within each string
 *   starting from each we need to sort. The assertion being that all prefixes
 *   before position `strstart` are already properly sorted.
 *
 * nsigbits
 *   Number of significant bits in the elements of `x`. This cannot exceed
 *   `8 * elemsize`, but could be less. This value is an assertion that all
 *   elements in `x` are in the range `[0; 2**nsigbits)`. The number of
 *   significant bits cannot be 0.
 *
 * shift
 *   The parameter of linear transform to be applied to each item in `x` to
 *   obtain the radix. That is, radix for element `i` is
 *       (x[i] >> shift)
 *   The `shift` can also be zero, indicating that the values themselves
 *   are the radixes (as in counting sort).
 *
 * nradixes
 *   Total number of possible radixes, equal to `1 << (nsigbits - shift)`.
 *
 * nth
 *   The number of threads used by OMP.
 *
 * nchunks, chunklen
 *   These variables describe how the total number of rows, `n`, will be split
 *   into smaller chunks for the parallel computation. In particular, the range
 *   `[0; n)` is divided into `nchunks` sub-ranges each except the last one
 *   having length `chunklen`. The last chunk will have the length
 *   `n - chunklen*(nchunks - 1)`. The number of chunks is >= 1, and `chunklen`
 *   is also positive.
 *
 * histogram
 *   Computed during the `build_histogram` step, this array will contain the
 *   histogram of data values, by chunk and by radix. More specifically, this
 *   is a `size_t*` array which is viewed as a 2D table. The `(i,k)`-th element
 *   of this array (where `i` is the chunk index, and `k` is radix) is located
 *   at index `(i * nradixes + k)` and represents the starting position within
 *   the output array where the elements with radix `k` and within the chunk `i`
 *   should be written. That is,
 *       histogram[i,k] = #{elements in chunks 0..i-1 with radix = k} +
 *                        #{elements in all chunks with radix < k}
 *   After the "reorder_data" step, this histogram is modified to contain values
 *       histogram[i,k] = #{elements in chunks 0..i with radix = k} +
 *                        #{elements in all chunks with radix < k}
 *
 * next_x
 *   When `shift > 0`, a single pass of the `radix_psort()` function will sort
 *   the data array only partially, and 1 or more extra passes will be needed to
 *   finish sorting. In this case the array `next_x` (of length `n`) will hold
 *   pre-sorted and potentially modified values of the original data array `x`.
 *   The array `next_x` is filled in the "reorder_data" step. If it was nullptr,
 *   the array will be allocated, otherwise its contents will be overwritten.
 *
 * next_o
 *   The reordered array `o` to be carried over to the next step, or to be
 *   returned as the final ordering in the end.
 *
 * next_elemsize
 *   Size in bytes of each element in `next_x`. This cannot be greater than
 *   `elemsize`, however `next_elemsize` can be 0.
 */
class SortContext {
  private:
    arr32_t order;
    arr32_t groups;

    void* x;
    void* next_x;
    int32_t* o;
    int32_t* next_o;
    size_t*  histogram;
    GroupGatherer gg;
    const uint8_t* strdata;
    const void* stroffs;
    size_t strstart;
    size_t n;
    size_t nth;
    size_t nchunks;
    size_t chunklen;
    size_t nradixes;
    size_t histogram_size;
    int8_t elemsize;
    int8_t next_elemsize;
    int8_t nsigbits;
    int8_t shift;
    int8_t strtype;
    bool use_order;
    int : 16;

  public:
  SortContext(const Column* col, bool make_groups) {
    next_x = nullptr;
    next_o = nullptr;
    histogram = nullptr;
    strdata = nullptr;
    histogram_size = 0;
    strtype = 0;

    nth = static_cast<size_t>(config::sort_nthreads);
    n = static_cast<size_t>(col->nrows);
    order = (col->rowindex()).extract_as_array32();
    use_order = (bool) order;
    if (!use_order) order.resize(n);
    o = order.data();
    if (make_groups) {
      groups.resize(n + 1);
      groups[0] = 0;
      gg.init(groups.data() + 1, 0);
    }
    // These will initialize `x`, `elemsize` and `nsigbits`, and also
    // `strdata`, `stroffs`, `strstart` for string columns
    SType stype = col->stype();
    switch (stype) {
      case ST_BOOLEAN_I1: _initB(col); break;
      case ST_INTEGER_I1: _initI<int8_t,  uint8_t>(col); break;
      case ST_INTEGER_I2: _initI<int16_t, uint16_t>(col); break;
      case ST_INTEGER_I4: _initI<int32_t, uint32_t>(col); break;
      case ST_INTEGER_I8: _initI<int64_t, uint64_t>(col); break;
      case ST_REAL_F4:    _initF<uint32_t>(col); break;
      case ST_REAL_F8:    _initF<uint64_t>(col); break;
      case ST_STRING_I4_VCHAR: _initS<int32_t>(col); break;
      case ST_STRING_I8_VCHAR: _initS<int64_t>(col); break;
      default:
        throw NotImplError() << "Unable to sort Column of stype " << stype;
    }
  }

  SortContext(const SortContext&) = delete;
  SortContext& operator=(const SortContext&) = delete;

  ~SortContext() {
    std::free(x);
    std::free(next_x);
    std::free(histogram);
    // Note: `o` is not owned by this class, see `initialize()`
    delete[] next_o;
  }


  void do_sort() {
    if (n <= config::sort_insert_method_threshold) {
      if (use_order) {
        kinsert_sort();
      } else {
        vinsert_sort();
      }
    } else {
      radix_psort();
    }
  }


  RowIndex get_result(Groupby* out_grps) {
    RowIndex res = RowIndex::from_array32(std::move(order));
    if (out_grps) {
      size_t ngrps = static_cast<size_t>(gg.size());
      groups.resize(ngrps + 1);
      *out_grps = Groupby(ngrps, groups.to_memoryrange());
    }
    return res;
  }



  //============================================================================
  // Data preparation
  //============================================================================

  /**
   * Boolean columns have only 3 distinct values: -128, 0 and 1. The transform
   * `(x + 0xBF) >> 6` converts these to 0, 2 and 3 respectively, provided that
   * the addition is done as addition of unsigned bytes (i.e. modulo 256).
   */
  void _initB(const Column* col) {
    const uint8_t* xi = static_cast<const uint8_t*>(col->data());
    uint8_t* xo = new uint8_t[n];
    x = static_cast<void*>(xo);
    elemsize = 1;
    nsigbits = 2;

    if (use_order) {
      #pragma omp parallel for schedule(static) num_threads(nth)
      for (size_t j = 0; j < n; j++) {
        uint8_t t = xi[o[j]] + 0xBF;
        xo[j] = t >> 6;
      }
    } else {
      #pragma omp parallel for schedule(static) num_threads(nth)
      for (size_t j = 0; j < n; j++) {
        // xi[j]+0xBF should be computed as uint8_t; by default C++ upcasts it
        // to int, which leads to wrong results after shift by 6.
        uint8_t t = xi[j] + 0xBF;
        xo[j] = t >> 6;
      }
    }
  }


  /**
   * For integer columns we subtract the min value, thus making the column
   * unsigned. Depending on the range of the values (max - min + 1), we cast
   * the data into an appropriate smaller type.
   */
  template <typename T, typename TU>
  void _initI(const Column* col) {
    auto icol = static_cast<const IntColumn<T>*>(col);
    xassert(sizeof(T) == sizeof(TU));
    T min = icol->min();
    T max = icol->max();
    nsigbits = sizeof(T) * 8;
    nsigbits -= dt::nlz(static_cast<TU>(max - min + 1));
    if (nsigbits > 32)      _initI_impl<T, TU, uint64_t>(icol, min);
    else if (nsigbits > 16) _initI_impl<T, TU, uint32_t>(icol, min);
    else if (nsigbits > 8)  _initI_impl<T, TU, uint16_t>(icol, min);
    else                    _initI_impl<T, TU, uint8_t >(icol, min);
  }

  template <typename T, typename TI, typename TO>
  void _initI_impl(const Column* col, T min) {
    TI una = static_cast<TI>(GETNA<T>());
    TI umin = static_cast<TI>(min);
    const TI* xi = static_cast<const TI*>(col->data());
    TO* xo = new TO[n];
    x = static_cast<void*>(xo);
    elemsize = sizeof(TO);

    if (use_order) {
      #pragma omp parallel for schedule(static) num_threads(nth)
      for (size_t j = 0; j < n; ++j) {
        TI t = xi[o[j]];
        xo[j] = t == una? 0 : static_cast<TO>(t - umin + 1);
      }
    } else {
      #pragma omp parallel for schedule(static) num_threads(nth)
      for (size_t j = 0; j < n; j++) {
        TI t = xi[j];
        xo[j] = t == una? 0 : static_cast<TO>(t - umin + 1);
      }
    }
  }


  /**
   * For float32/64 we need to carefully manipulate the bits in order to present
   * them in the correct order as uint32/64. At bit level, the structure of
   * IEEE754-1985 float is the following (first 1 bit is the sign, next 8 bits
   * are the exponent, last 23 bits represent the significand):
   *      Bits (uint32 value)          Float value
   *      0 00 000000                  +0
   *      0 00 000001 - 0 00 7FFFFF    Denormal numbers (positive)
   *      0 01 000000 - 0 FE 7FFFFF    +1*2^-126 .. +1.7FFFFF*2^+126
   *      0 FF 000000                  +Inf
   *      0 FF 000001 - 0 FF 7FFFFF    NaNs (positive)
   *      1 00 000000                  -0
   *      1 00 000001 - 1 00 7FFFFF    Denormal numbers (negative)
   *      1 01 000000 - 1 FE 7FFFFF    -1*2^-126 .. -1.7FFFFF*2^+126
   *      1 FF 000000                  -Inf
   *      1 FF 000001 - 1 FF 7FFFFF    NaNs (negative)
   * In order to put these values into correct order, we'll do the following
   * transform:
   *      (1) numbers with sign bit = 0 will turn the sign bit on.
   *      (2) numbers with sign bit = 1 will be XORed with 0xFFFFFFFF
   *      (3) all NAs/NaNs will be converted to 0
   *
   * Float64 is similar: 1 bit for the sign, 11 bits of exponent, and finally
   * 52 bits of the significand.
   *
   * See also:
   *      https://en.wikipedia.org/wiki/Float32
   *      https://en.wikipedia.org/wiki/Float64
   */
  template <typename TO>
  void _initF(const Column* col) {
    const TO* xi = static_cast<const TO*>(col->data());
    TO* xo = new TO[n];
    x = static_cast<void*>(xo);
    elemsize = sizeof(TO);
    nsigbits = elemsize * 8;

    constexpr TO EXP
      = static_cast<TO>(sizeof(TO) == 8? 0x7FF0000000000000ULL : 0x7F800000);
    constexpr TO SIG
      = static_cast<TO>(sizeof(TO) == 8? 0x000FFFFFFFFFFFFFULL : 0x007FFFFF);
    constexpr TO SBT
      = static_cast<TO>(sizeof(TO) == 8? 0x8000000000000000ULL : 0x80000000);
    constexpr int SHIFT = sizeof(TO) * 8 - 1;

    if (use_order) {
      #pragma omp parallel for schedule(static) num_threads(nth)
      for (size_t j = 0; j < n; j++) {
        TO t = xi[o[j]];
        xo[j] = ((t & EXP) == EXP && (t & SIG) != 0)
                ? 0 : t ^ (SBT | -(t>>SHIFT));
      }
    } else {
      #pragma omp parallel for schedule(static) num_threads(nth)
      for (size_t j = 0; j < n; j++) {
        TO t = xi[j];
        xo[j] = ((t & EXP) == EXP && (t & SIG) != 0)
                ? 0 : t ^ (SBT | -(t>>SHIFT));
      }
    }
  }


  /**
   * For strings, we fill array `x` with the values of the first 2 characters in
   * each string. We also set up auxiliary variables `strdata`, `stroffs`,
   * `strstart` and `strtype`.
   *
   * More specifically, for each string item, if it is NA then we map it to 0;
   * if it is an empty string we map it to 1, otherwise we map it to `ch[i] + 2`
   * where `ch[i]` is the i-th character of the string. This doesn't overflow
   * because in UTF-8 the largest legal byte is 0xF7.
   */
  template <typename T>
  void _initS(const Column* col) {
    auto scol = static_cast<const StringColumn<T>*>(col);
    strdata = reinterpret_cast<const uint8_t*>(scol->strdata());
    const T* offs = scol->offsets();
    stroffs = static_cast<const void*>(offs);
    strtype = sizeof(T) / 4;
    strstart = 0;
    uint8_t* xo = new uint8_t[n];
    x = static_cast<void*>(xo);
    elemsize = 1;
    nsigbits = 8;

    T maxlen = 0;
    #pragma omp parallel for schedule(static) num_threads(nth) \
            reduction(max:maxlen)
    for (size_t j = 0; j < n; ++j) {
      int32_t k = use_order? o[j] : static_cast<int32_t>(j);
      T offend = offs[k];
      if (offend < 0) {  // NA
        xo[j] = 0;
      } else {
        T offstart = std::abs(offs[k - 1]);
        T len = offend - offstart;
        xo[j] = len > 0? strdata[offstart] + 2 : 1;
        if (len > maxlen) maxlen = len;
      }
    }
    next_elemsize = (maxlen > 1);
  }


  //============================================================================
  // Radix sorting parameters
  //============================================================================

  /**
   * Determine how the input should be split into chunks: at least as many
   * chunks as the number of threads, unless the input array is too small
   * and chunks become too small in size. We want to have more than 1 chunk
   * per thread so as to reduce delays caused by uneven execution time among
   * threads, on the other hand too many chunks should be avoided because that
   * would increase the time needed to combine the results from different
   * threads.
   * Also compute the desired radix size, as a function of `nsigbits` (number of
   * significant bits in the data column).
   *
   * SortContext inputs:
   *      n, nsigbits
   *
   * SortContext outputs:
   *      nth, nchunks, chunklen, shift, nradixes
   */
  void determine_sorting_parameters() {
    size_t nch = nth * config::sort_thread_multiplier;
    chunklen = std::max((n - 1) / nch + 1,
                        config::sort_max_chunk_length);
    nchunks = (n - 1)/chunklen + 1;

    int8_t nradixbits = nsigbits < config::sort_max_radix_bits
                        ? nsigbits : config::sort_over_radix_bits;
    shift = nsigbits - nradixbits;
    nradixes = 1 << nradixbits;

    if (!strdata) {
      // The remaining number of sig.bits is `shift`. Thus, this value will
      // determine the `next_elemsize`.
      next_elemsize = shift > 32? 8 :
                      shift > 16? 4 :
                      shift > 0? 2 : 0;
    }
  }



  //============================================================================
  // Histograms (for radix sort)
  //============================================================================

  /**
   * Calculate initial histograms of values in `x`. Specifically, we're creating
   * the `histogram` table which has `nchunks` rows and `nradix` columns. Cell
   * `[i,j]` in this table will contain the count of values `x` within the chunk
   * `i` such that the topmost `nradixbits` of `x` are equal to `j`. After that
   * the values are cumulated across all `j`s (i.e. in the end the histogram
   * will contain cumulative counts of values in `x`).
   */
  void build_histogram() {
    size_t counts_size = nchunks * nradixes;
    if (histogram_size < counts_size) {
      histogram = static_cast<size_t*>(
        std::realloc(histogram, counts_size * sizeof(size_t)));
      histogram_size = counts_size;
    }
    std::memset(histogram, 0, counts_size * sizeof(size_t));
    switch (elemsize) {
      case 1: _histogram_gather<uint8_t>();  break;
      case 2: _histogram_gather<uint16_t>(); break;
      case 4: _histogram_gather<uint32_t>(); break;
      case 8: _histogram_gather<uint64_t>(); break;
    }
    _histogram_cumulate();
  }

  template<typename T> void _histogram_gather() {
    T* tx = static_cast<T*>(x);
    #pragma omp parallel for schedule(dynamic) num_threads(nth)
    for (size_t i = 0; i < nchunks; ++i) {
      size_t* cnts = histogram + (nradixes * i);
      size_t j0 = i * chunklen;
      size_t j1 = std::min(j0 + chunklen, n);
      for (size_t j = j0; j < j1; ++j) {
        cnts[tx[j] >> shift]++;
      }
    }
  }

  void _histogram_cumulate() {
    size_t cumsum = 0;
    size_t counts_size = nchunks * nradixes;
    for (size_t j = 0; j < nradixes; ++j) {
      for (size_t r = j; r < counts_size; r += nradixes) {
        size_t t = histogram[r];
        histogram[r] = cumsum;
        cumsum += t;
      }
    }
  }



  //============================================================================
  // Reorder data (for radix sort)
  //============================================================================

  /**
   * Perform the main radix shuffle, filling in arrays `next_o` and `next_x`.
   * The array `next_o` will contain the original row numbers of the values in
   * `x` such that `x[next_o]` is sorted with respect to the most significant
   * bits. The `next_x` array will contain the sorted elements of `x`, with MSB
   * bits already removed -- we need it mostly for page-efficiency at later
   * stages of the algorithm.
   *
   * During execution of this step we also modify the `histogram` array so that
   * by the end of the step each cell in `histogram` will contain the offset
   * past the *last* item within that cell. In particular, the last row of the
   * `histogram` table will contain end-offsets of output regions corresponding
   * to each radix value.
   */
  void reorder_data() {
    if (!next_x && next_elemsize) {
      size_t sz = static_cast<size_t>(next_elemsize);
      // Allocate as `int64_t` to ensure 8-byte alignment
      next_x = new int64_t[(n * sz + 7) / 8];
    }
    if (!next_o) {
      next_o = new int32_t[n];
    }
    if (strtype) {
      if (next_x) {
        if (strtype == 1) _reorder_str<int32_t>();
        else              _reorder_str<int64_t>();
      } else _reorder_impl<uint8_t, char, false>();
    } else {
      switch (elemsize) {
        case 8:
          xassert(next_elemsize == 8 || next_elemsize == 4);
          if (next_elemsize == 8) _reorder_impl<uint64_t, uint64_t, true>();
          if (next_elemsize == 4) _reorder_impl<uint64_t, uint32_t, true>();
          break;
        case 4:
          xassert(next_elemsize == 4 || next_elemsize == 2);
          if (next_elemsize == 4) _reorder_impl<uint32_t, uint32_t, true>();
          if (next_elemsize == 2) _reorder_impl<uint32_t, uint16_t, true>();
          break;
        case 2:
          xassert(next_elemsize == 2 || next_elemsize == 0);
          if (next_elemsize == 2) _reorder_impl<uint16_t, uint16_t, true>();
          if (next_elemsize == 0) _reorder_impl<uint16_t, uint8_t, false>();
          break;
        case 1:
          xassert(next_elemsize == 0);
          _reorder_impl<uint8_t, uint8_t, false>();
          break;
      }
    }
    std::swap(x, next_x);
    std::swap(o, next_o);
    std::swap(elemsize, next_elemsize);
    use_order = true;
  }

  template<typename TI, typename TO, bool OUT> void _reorder_impl() {
    TI* xi = static_cast<TI*>(x);
    TO* xo;
    TI mask;
    if (OUT) {
      xo = static_cast<TO*>(next_x);
      mask = static_cast<TI>((1ULL << shift) - 1);
    }
    #pragma omp parallel for schedule(dynamic) num_threads(nth)
    for (size_t i = 0; i < nchunks; ++i) {
      size_t j0 = i * chunklen;
      size_t j1 = std::min(j0 + chunklen, n);
      size_t* tcounts = histogram + (nradixes * i);
      for (size_t j = j0; j < j1; ++j) {
        size_t k = tcounts[xi[j] >> shift]++;
        xassert(k < n);
        next_o[k] = use_order? o[j] : static_cast<int32_t>(j);
        if (OUT) {
          xo[k] = static_cast<TO>(xi[j] & mask);
        }
      }
    }
    xassert(histogram[nchunks * nradixes - 1] == n);
  }

  template <typename T> void _reorder_str() {
    uint8_t* xi = static_cast<uint8_t*>(x);
    uint8_t* xo = static_cast<uint8_t*>(next_x);
    const T sstart = static_cast<T>(strstart) + 1;
    const T* soffs = static_cast<const T*>(stroffs);

    T maxlen = 0;
    #pragma omp parallel for schedule(dynamic) num_threads(nth) \
            reduction(max:maxlen)
    for (size_t i = 0; i < nchunks; ++i) {
      size_t j0 = i * chunklen;
      size_t j1 = std::min(j0 + chunklen, n);
      size_t* tcounts = histogram + (nradixes * i);
      for (size_t j = j0; j < j1; ++j) {
        size_t k = tcounts[xi[j]]++;
        xassert(k < n);
        int32_t w = use_order? o[j] : static_cast<int32_t>(j);
        T offend = soffs[w];
        T offstart = std::abs(soffs[w - 1]) + sstart;
        T len = offend - offstart;
        xo[k] = len > 0? strdata[offstart] + 2 : 1;
        next_o[k] = w;
        if (len > maxlen) maxlen = len;
      }
    }
    next_elemsize = maxlen > 0;
    xassert(histogram[nchunks * nradixes - 1] == n);
  }



  //============================================================================
  // Radix sort
  //============================================================================

  /**
   * Sort array `o` of length `n` by values `x`, using MSB Radix sort algorithm.
   * Array `o` will be modified in-place. If `o` is nullptr, then it will be
   * allocated and filled with values of `range(n)`.
   *
   * SortContext inputs:
   *   x:      buffer of size `elemsize * n`
   *   next_x: nullptr, or buffer of size `next_elemsize * n`
   *   o:      nullptr, or array of `int32_t`s of length `n`
   *   next_o: nullptr, or array of `int32_t`s of length `n`
   *   elemsize
   *   next_elemsize: (may be zero)
   *   nsigbits
   *
   * SortContext outputs:
   *   o: If nullptr, this array will be allocated; otherwise its contents
   *      will be modified in-place.
   *   x, next_x, next_o, histogram: These arrays may be allocated, or their
   *      contents may be altered arbitrarily.
   */
  void radix_psort() {
    int32_t* ores = o;
    determine_sorting_parameters();
    build_histogram();
    reorder_data();

    if (elemsize) {
      // If after reordering there are still unsorted elements in `x`, then
      // sort them recursively.
      if (groups) {
        _radix_recurse<true>();
      } else {
        _radix_recurse<false>();
      }
    } else if (groups) {
      // Otherwise groups can be computed directly from the histogram
      gg.from_histogram(histogram, nchunks, nradixes);
    }

    // Done. Save to array `o` the computed ordering of the input vector `x`.
    if (ores && o != ores) {
      std::memcpy(ores, o, n * sizeof(int32_t));
      next_o = o;
      o = ores;
    }
  }

  /**
   * Helper for radix sorting function.
   *
   * At this point the input array is already partially sorted, and the
   * elements that remain to be sorted are collected into contiguous
   * chunks. For example if `shift` is 2, then `x` may be:
   *     na na | 0 2 1 3 1 | 2 | 1 1 3 0 | 3 0 0 | 2 2 2 2 2 2
   *
   * For each distinct radix there is a "range" within `x` that
   * contains values corresponding to that radix. The values in `x`
   * have their most significant bits already removed, since they are
   * constant within each radix range. The array `x` is accompanied
   * by array `o` which carries the original row numbers of each
   * value. Once we sort `o` by the values of `x` within each
   * radix range, our job will be complete.
   */
  template <bool make_groups>
  void _radix_recurse() {
    // Save some of the variables in SortContext that we will be modifying
    // in order to perform the recursion.
    size_t   _n        = n;
    void*    _x        = x;
    void*    _next_x   = next_x;
    int32_t* _o        = o;
    int32_t* _next_o   = next_o;
    int8_t   _elemsize = elemsize;
    int8_t   _nsigbits = nsigbits;
    size_t   _nradixes = nradixes;
    size_t   _strstart = strstart;
    int32_t  ggoff0    = make_groups? gg.cumulative_size() : 0;
    int32_t* ggdata0   = make_groups? gg.data() : nullptr;
    size_t   zelemsize = static_cast<size_t>(elemsize);

    // First, determine the sizes of ranges corresponding to each radix that
    // remain to be sorted. The previous step left us with the `histogram`
    // array containing cumulative sizes of these ranges, so all we need is
    // to diff that array.
    size_t* rrendoffsets = histogram + (nchunks - 1) * nradixes;
    radix_range* rrmap = new radix_range[nradixes];
    for (size_t i = 0; i < nradixes; i++) {
      size_t start = i? rrendoffsets[i-1] : 0;
      size_t end = rrendoffsets[i];
      xassert(start <= end);
      rrmap[i].size   = end - start;
      rrmap[i].offset = start;
    }

    // At this point the distribution of radix range sizes may or may not
    // be uniform. If the distribution is uniform (i.e. roughly same number
    // of elements in each range), then the best way to proceed is to let
    // all threads process the ranges in-parallel, each thread working on
    // its own range. However if the distribution is highly skewed (i.e.
    // one or several overly large ranges), then such approach will be
    // suboptimal. In the worst-case scenario we would have single thread
    // processing almost the entire array while other threads are idling.
    // In order to combat such "skew", we first process all "large" radix
    // ranges one-at-a-time and sorting each of them in a multithreaded way.
    // How big should a radix range be to be considered "large"? If there
    // are `n` elements in the array, and the array is split into `k` ranges
    // then the lower bound for the size of the largest range is `n/k`. In
    // fact, if the largest range's size is exactly `n/k`, then all other
    // ranges are forced to have the same sizes (which is an ideal
    // situation). In practice we deem a range to be "large" if its size is
    // more then `2n/k`.
    // size_t rrlarge = 2 * _n / nradixes;
    constexpr size_t GROUPED = size_t(1) << 63;
    size_t size0 = 0;
    size_t nsmallgroups = 0;
    size_t rrlarge = config::sort_insert_method_threshold;  // for now
    xassert(GROUPED > rrlarge);

    strstart = _strstart + 1;
    nsigbits = strdata? 8 : shift;

    for (size_t rri = 0; rri < _nradixes; ++rri) {
      size_t sz = rrmap[rri].size;
      if (sz > rrlarge) {
        size_t off = rrmap[rri].offset;
        n = sz;
        x = add_ptr(_x, off * zelemsize);
        o = _o + off;
        next_x = add_ptr(_next_x, off * zelemsize);
        next_o = _next_o + off;
        elemsize = _elemsize;
        if (make_groups) {
          gg.init(ggdata0 + off, ggoff0 + static_cast<int32_t>(off));
        }
        radix_psort();
        if (make_groups) {
          rrmap[rri].size = static_cast<size_t>(gg.size()) | GROUPED;
        }
      } else {
        nsmallgroups += (sz > 1);
        if (sz > size0) size0 = sz;
      }
    }

    n = _n;
    x = _x;
    o = _o;
    next_x = _next_x;
    next_o = _next_o;
    strstart = _strstart;
    gg.init(ggdata0, ggoff0);
    nsigbits = _nsigbits;

    // Finally iterate over all remaining radix ranges, in-parallel, and
    // sort each of them independently using a simpler insertion sort
    // method.
    size_t nthreads = std::min(nth, nsmallgroups);
    int32_t* tmp = nullptr;
    bool own_tmp = false;
    if (size0) {
      // size_t size_all = size0 * nthreads * sizeof(int32_t);
      // if ((size_t)_next_elemsize * _n <= size_all) {
      //   tmp = (int32_t*)_x;
      // } else {
      own_tmp = true;
      tmp = new int32_t[size0 * nthreads];
      // }
    }
    #pragma omp parallel num_threads(nthreads)
    {
      int tnum = omp_get_thread_num();
      int32_t* oo = tmp + tnum * static_cast<int32_t>(size0);
      GroupGatherer tgg;

      #pragma omp for schedule(dynamic)
      for (size_t i = 0; i < _nradixes; ++i) {
        size_t zn  = rrmap[i].size;
        size_t off = rrmap[i].offset;
        if (zn > rrlarge) {
          rrmap[i].size = zn & ~GROUPED;
        } else if (zn > 1) {
          int32_t  tn = static_cast<int32_t>(zn);
          void*    tx = static_cast<char*>(_x) + off * zelemsize;
          int32_t* to = _o + off;
          if (make_groups) {
            tgg.init(ggdata0 + off, static_cast<int32_t>(off) + ggoff0);
          }
          if (strtype == 0) {
            switch (_elemsize) {
              case 1: insert_sort_keys<>(static_cast<uint8_t*>(tx), to, oo, tn, tgg); break;
              case 2: insert_sort_keys<>(static_cast<uint16_t*>(tx), to, oo, tn, tgg); break;
              case 4: insert_sort_keys<>(static_cast<uint32_t*>(tx), to, oo, tn, tgg); break;
              case 8: insert_sort_keys<>(static_cast<uint64_t*>(tx), to, oo, tn, tgg); break;
            }
          } else if (strtype == 1) {
            const int32_t* soffs = static_cast<const int32_t*>(stroffs);
            int32_t ss = static_cast<int32_t>(_strstart + 1);
            insert_sort_keys_str(strdata, soffs, ss, to, oo, tn, tgg);
          } else {
            const int64_t* soffs = static_cast<const int64_t*>(stroffs);
            int64_t ss = static_cast<int64_t>(_strstart + 1);
            insert_sort_keys_str(strdata, soffs, ss, to, oo, tn, tgg);
          }
          if (make_groups) {
            rrmap[i].size = static_cast<size_t>(tgg.size());
          }
        } else if (zn == 1 && make_groups) {
          ggdata0[off] = static_cast<int32_t>(off) + ggoff0 + 1;
          rrmap[i].size = 1;
        }
      }
    }

    // Consolidate groups into a single contiguous chunk
    if (make_groups) {
      gg.from_chunks(rrmap, _nradixes);
    }

    delete[] rrmap;
    if (own_tmp) delete[] tmp;
  }



  //============================================================================
  // Insert sort
  //============================================================================

  void kinsert_sort() {
    arr32_t tmparr(n);
    int32_t* tmp = tmparr.data();
    int32_t nn = static_cast<int32_t>(n);
    if (strtype == 0) {
      switch (elemsize) {
        case 1: _insert_sort_keys<uint8_t >(tmp); break;
        case 2: _insert_sort_keys<uint16_t>(tmp); break;
        case 4: _insert_sort_keys<uint32_t>(tmp); break;
        case 8: _insert_sort_keys<uint64_t>(tmp); break;
      }
    } else if (strtype == 1) {
      const int32_t* soffs = static_cast<const int32_t*>(stroffs);
      insert_sort_keys_str(strdata, soffs, int32_t(0), o, tmp, nn, gg);
    } else {
      const int64_t* soffs = static_cast<const int64_t*>(stroffs);
      insert_sort_keys_str(strdata, soffs, int64_t(0), o, tmp, nn, gg);
    }
  }

  void vinsert_sort() {
    if (strtype == 0) {
      switch (elemsize) {
        case 1: _insert_sort_values<uint8_t >(); break;
        case 2: _insert_sort_values<uint16_t>(); break;
        case 4: _insert_sort_values<uint32_t>(); break;
        case 8: _insert_sort_values<uint64_t>(); break;
      }
    } else if (strtype == 1) {
      int32_t nn = static_cast<int32_t>(n);
      const int32_t* soffs = static_cast<const int32_t*>(stroffs);
      insert_sort_values_str(strdata, soffs, int32_t(0), o, nn, gg);
    } else {
      int32_t nn = static_cast<int32_t>(n);
      const int64_t* soffs = static_cast<const int64_t*>(stroffs);
      insert_sort_values_str(strdata, soffs, int64_t(0), o, nn, gg);
    }
  }

  template <typename T> void _insert_sort_keys(int32_t* tmp) {
    T* xt = static_cast<T*>(x);
    int32_t nn = static_cast<int32_t>(n);
    insert_sort_keys(xt, o, tmp, nn, gg);
  }

  template <typename T> void _insert_sort_values() {
    T* xt = static_cast<T*>(x);
    int32_t nn = static_cast<int32_t>(n);
    insert_sort_values(xt, o, nn, gg);
  }

};



//==============================================================================
// Main sorting routines
//==============================================================================

/**
 * Sort the column, and return its ordering as a RowIndex object. This function
 * will choose the most appropriate algorithm for sorting. The data in column
 * `col` will not be modified.
 *
 * The function returns nullptr if there is a runtime error (for example an
 * intermediate buffer cannot be allocated).
 */
RowIndex DataTable::sortby(const arr32_t& colindices, Groupby* out_grps) const
{
  if (colindices.size() != 1) {
    throw NotImplError() << "Sorting by multiple columns is not supported yet";
  }
  if (nrows > INT32_MAX) {
    throw NotImplError() << "Cannot sort a datatable with " << nrows << " rows";
  }
  if (rowindex.isarr64() || rowindex.length() > INT32_MAX ||
      rowindex.max() > INT32_MAX) {
    throw NotImplError() << "Cannot sort a datatable which is based on a "
                            "datatable with >2**31 rows";
  }
  Column* col0 = columns[colindices[0]];
  return col0->sort(out_grps);
}


static RowIndex sort_tiny(const Column* col, Groupby* out_grps) {
  int64_t i = col->rowindex().nth(0);
  if (out_grps) {
    *out_grps = Groupby::single_group(col->nrows);
  }
  return RowIndex::from_slice(i, col->nrows, 1);
}


RowIndex Column::sort(Groupby* out_grps) const {
  if (nrows <= 1) {
    return sort_tiny(this, out_grps);
  }
  SortContext sc(this, (out_grps != nullptr));
  sc.do_sort();
  return sc.get_result(out_grps);
}
