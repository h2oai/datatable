#!/usr/bin/groovy

@Library('test-shared-library@1.2') _

import ai.h2o.ci.Utils

def utilsLib = new Utils()

properties([
        parameters([
                booleanParam(name: 'BUILD_PPC64LE', defaultValue: true, description: '[BUILD] Trigger build of PPC64le artifacts'),
                booleanParam(name: 'DISABLE_TESTS', defaultValue: false, description: '[BUILD] Disable tests'),
                booleanParam(name: 'DISABLE_COVERAGE', defaultValue: false, description: '[BUILD] Disable coverage')
        ]),
        buildDiscarder(logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '180', numToKeepStr: ''))
])

RELEASE_BRANCH_PREFIX = 'rel-'

CREDS_ID = 'h2o-ops-personal-auth-token'

def DOCKER_IMAGE_TAG = '0.3.2-PR-996.3'

def X86_64_LINUX_DOCKER_IMAGE = "docker.h2o.ai/opsh2oai/datatable-build-x86_64_ubuntu:${DOCKER_IMAGE_TAG}"

EXPECTED_SHAS = [
        files : [
                'ci/Dockerfile-ubuntu.in' : '47087215fe899c38e12037e2d91dd4202988e98c',
                'ci/Dockerfile-centos7.in': 'cac8444c631555cff6b291f30cbc330c917cb0fe',
        ]
]

// Default mapping
def platformDefaults = [
        env      : [],
        pythonBin: 'python',
        test     : true,
        coverage : true,
]

// Default mapping for CentOS environments
def centosDefaults = [
        env      : ['LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/h2oai/dai/python/lib/'],
]

def PPC64LE_PLATFORM = 'ppc64le_linux'
def PPC64LE_BUILD_CONF = platformDefaults + centosDefaults + [
        node       : 'ibm-power',
        coverage   : false,
        test       : false,
        dockerImage: "docker.h2o.ai/opsh2oai/datatable-build-ppc64le_centos7:${DOCKER_IMAGE_TAG}"
]

def LINUX_PLATFORM = 'x86_64_linux'

// Build platforms parameters
def BUILD_MATRIX = [
        // Linux build definition
        (LINUX_PLATFORM)   : platformDefaults + [
                node       : 'docker && linux',
                dockerImage: X86_64_LINUX_DOCKER_IMAGE
        ],
        // CentOS 7 build definition
        x86_64_centos7: platformDefaults + centosDefaults + [
                node       : 'docker && linux',
                coverage   : false,
                dockerImage: "docker.h2o.ai/opsh2oai/datatable-build-x86_64_centos7:${DOCKER_IMAGE_TAG}"
        ],
        // OSX
        x86_64_macos  : platformDefaults + [
                node: 'osx',
                env : [
                        "LLVM4=/usr/local/opt/llvm",
                        "CI_EXTRA_COMPILE_ARGS=-DDISABLE_CLOCK_REALTIME"
                ],
        ]
]

if ((!isPrJob() && params.BUILD_PPC64LE) || isRelease()) {
    BUILD_MATRIX[PPC64LE_PLATFORM] = PPC64LE_BUILD_CONF
    manager.addBadge("success.gif", "PPC64LE build triggered!")
}

// Virtualenv used for build and coverage
def DEFAULT_PY36_ENV = 'datatable-py36-with-pandas'
def DEFAULT_PY35_ENV = 'datatable-py35-with-pandas'
// Virtualenvs used for PR testing
PR_TEST_ENVIRONMENTS = [
        DEFAULT_PY35_ENV,
        DEFAULT_PY36_ENV
]
// Virtualenvs used for nightly testing
NIGHTLY_TEST_ENVIRONMENTS = PR_TEST_ENVIRONMENTS + [
        'datatable-py36',
        'datatable-py36-with-numpy'
]

// Computed version suffix
def CI_VERSION_SUFFIX = utilsLib.getCiVersionSuffix()

// Global project build trigger filled in init stage
def project

// Needs invocation of larger tests
def needsLargerTest

// Paths should be absolute
SOURCE_DIR = "/home/0xdiag"
TARGET_DIR = "/tmp/pydatatable_large_data"


BUCKET = 'h2o-release'
STABLE_FOLDER = 'datatable/stale'
S3_URL_STABLE = "s3://${BUCKET}/${STABLE_FOLDER}"
HTTPS_URL_STABLE = "https://${BUCKET}.s3.amazonaws.com/${STABLE_FOLDER}"

// Data map for linking into container
def linkMap = [
        "Data"     : "h2oai-benchmarks/Data",
        "smalldata": "h2o-3/smalldata",
        "bigdata"  : "h2o-3/bigdata",
        "fread"    : "h2o-3/fread",
]

def dockerArgs = createDockerArgs(linkMap, SOURCE_DIR, TARGET_DIR)

/////////////////////////////////////
// Build, Coverage and Test stages //
/////////////////////////////////////
def stages = [:]
BUILD_MATRIX.each { platform, config ->
    stages[platform] = {
        node(config.node) {
            final def buildDir = 'build'

            cleanWs()
            def buildStageName = "Build ${platform}"
            withCustomCommitStates(scm, CREDS_ID, buildStageName) {
                stage(buildStageName) {
                    dir(buildDir) {
                        unstash 'datatable-sources'
                    }
                    // Specify build Closure
                    def buildClosure = {
                        project.buildAll(buildDir, platform, CI_VERSION_SUFFIX, getVenvActivationCmd(platform, DEFAULT_PY36_ENV), getVenvActivationCmd(platform, DEFAULT_PY35_ENV), BUILD_MATRIX[platform]['env'])
                    }
                    callInEnv(config.dockerImage, dockerArgs, buildClosure)
                }
            }


            if (config.coverage && !params.DISABLE_COVERAGE) {
                final def coverageDir = "coverage-${platform}"
                dir(coverageDir) {
                    unstash 'datatable-sources'
                }
                def coverageStageName = "Coverage ${platform}"
                coverageStageName = appendLargeTestsSuffixIfRequired(coverageStageName, needsLargerTest)
                withCustomCommitStates(scm, CREDS_ID, coverageStageName) {
                    stage(coverageStageName) {
                        def coverageClosure = {
                            project.coverage(coverageDir, platform, getVenvActivationCmd(platform, DEFAULT_PY36_ENV), BUILD_MATRIX[platform]['env'], false, TARGET_DIR)
                        }
                        callInEnv(config.dockerImage, dockerArgs, coverageClosure)
                    }
                }
            } else {
                stage("Coverage ${platform} - SKIPPED") {
                    echo "SKIPPED"
                }
            }

            boolean failure = false
            getRelevantTestEnvs().each { testEnv ->
                final def testDir = "test-${platform}-${testEnv}"
                try {
                    def testStageName = "Test ${platform} with ${testEnv}"
                    if (config.test && !params.DISABLE_TESTS) {
                        testStageName = appendLargeTestsSuffixIfRequired(testStageName, needsLargerTest)
                        withCustomCommitStates(scm, CREDS_ID, testStageName) {
                            stage(testStageName) {
                                dir(testDir) {
                                    unstash 'datatable-sources'
                                }
                                def testClosure = {
                                    project.test(testDir, platform, getVenvActivationCmd(platform, testEnv), BUILD_MATRIX[platform]['env'], needsLargerTest, TARGET_DIR)
                                }
                                callInEnv(config.dockerImage, dockerArgs, testClosure)
                            }
                        }
                    } else {
                        stage(testStageName) {
                            echo "SKIPPED"
                        }
                    }
                } catch (e) {
                    failure = true
                    StringWriter sw = new StringWriter()
                    e.printStackTrace(new PrintWriter(sw))
                    echo sw.toString()
                }
            }
            if (failure) {
                error 'There were one or more test failures. Please check the stacktrace above.'
            }
        }
    }
}

////////////////////////
// Main pipeline code //
////////////////////////
ansiColor('xterm') {
    timestamps {
        if (isPrJob()) {
            cancelPreviousBuilds()
        }

        // Checkout, apply patch and stash sources
        node('linux') {
            stage('Init') {
                checkout scm
                buildInfo(env.BRANCH_NAME, isRelease())
                project = load 'ci/default.groovy'
                if (isRelease()) {
                    CI_VERSION_SUFFIX = ''
                }
                needsLargerTest = isModified("(py_)?fread\\..*|__version__\\.py")
                if (needsLargerTest) {
                    echo "Large tests will be executed for this build"
                    manager.addBadge("warning.gif", "Large tests required")
                }
                stash 'datatable-sources'
                stash includes: "CHANGELOG.md", name: 'CHANGELOG'

                docker.image(X86_64_LINUX_DOCKER_IMAGE).inside {
                    def dockerfileSHAsString = ""
                    EXPECTED_SHAS.files.each { filename, sha ->
                        dockerfileSHAsString += "${sha}\t${filename}\n"
                    }
                    try {
                        sh """
                            echo "${dockerfileSHAsString}" > dockerfiles.sha
                            sha1sum -c dockerfiles.sha
                            rm -f dockerfiles.sha
                        """
                    } catch (e) {
                        error "Dockerfiles do not have expected checksums. Please make sure, you have built the " +
                                "new images using the Jenkins pipeline and that you have changed the required " +
                                "fields in this pipeline."
                        throw e
                    }
                }
            }
        }

        // Build, Coverage and Test stages for all platforms
        parallel stages

        def versionText = null
        def publishS3Dir = 'publish-s3'
        // If building master or rel-* branch, publish to S3
        if (doPublish()) {
            // Build and archive sdist wheel and read information required for following stages.
            node('docker && !mr-0xc8') {
                final def buildDir = 'build-sdist'
                stage('Build sdist') {
                    dir(buildDir) {
                        unstash 'datatable-sources'
                    }
                    // read additional information for following stages
                    unstash 'VERSION'
                    versionText = readFile("build/dist/VERSION.txt").trim()

                    def buildClosure = {
                        project.buildSDist(buildDir, CI_VERSION_SUFFIX, getVenvActivationCmd(LINUX_PLATFORM, DEFAULT_PY36_ENV))
                    }
                    callInEnv(X86_64_LINUX_DOCKER_IMAGE, dockerArgs, buildClosure)
                }
            }

            if (versionText == null || versionText.trim() == '') {
                error 'Version cannot be read'
            }

            // Always publish to artifacts.h2o.ai
            node('master') {
                stage('Publish snapshot to S3') {
                    cleanWs()
                    dumpInfo()
                    project.pullFilesFromArch('build/dist/**/*.whl, build-sdist/dist/**/*.tar.gz', "${publishS3Dir}/dist")
                    BUILD_MATRIX.each { platform, config ->
                        uploadToS3("${publishS3Dir}/dist/*${project.getWheelPlatformName(platform)}*", versionText, project.getS3PlatformName(platform))
                        uploadToS3("${publishS3Dir}/dist/*.tar.gz", versionText, project.getS3PlatformName(platform))
                    }
                }
            }

            // If we are building rel-* branch, ask for user confirmation in order to promote this build. Given approval,
            // artifacts will be copied to h2o-release and a GitHub release will be created.
            if (isRelease()) {
                timeout(unit: 'HOURS', time: 24) {
                    input('Promote build?')
                }
                node('master') {
                    cleanWs()
                    // git repository and CHANGELOG.md must be present in order to create GitHub Release
                    dir(publishS3Dir) {
                        checkout scm
                        unstash 'CHANGELOG'
                    }
                    project.pullFilesFromArch('build/dist/**/*.whl, build-sdist/dist/**/*.tar.gz', "${publishS3Dir}/dist")

                    final def publishPublicS3StageName = 'Publish to public S3'
                    stage(publishPublicS3StageName) {
                        dumpInfo()
                        docker.withRegistry("https://docker.h2o.ai", "docker.h2o.ai") {
                            withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', credentialsId: "awsArtifactsUploader"]]) {
                                docker.image("docker.h2o.ai/s3cmd").inside {
                                    sh """
                                        s3cmd put -P ${publishS3Dir}/dist/*.whl ${getS3TargetUrl(versionText)}
                                        s3cmd put -P ${publishS3Dir}/dist/*.tar.gz ${getS3TargetUrl(versionText)}
                                    """
                                }
                            }
                        }
                    }

                    stage('GitHub Release') {
                        dumpInfo()
                        docker.withRegistry("https://docker.h2o.ai", "docker.h2o.ai") {
                            docker.image('docker.h2o.ai/opsh2oai/hub').inside("--init -v /home/jenkins/.ssh:/home/jenkins/.ssh:ro -v /home/jenkins/.gitconfig:/home/jenkins/.gitconfig:ro") {
                                withCredentials([string(credentialsId: CREDS_ID, variable: 'GITHUB_TOKEN')]) {
                                    final def releaseMsgFile = "release-msg.md"
                                    def releaseMsg = """v${versionText}

${project.getChangelogPartForVersion(versionText, "${publishS3Dir}/CHANGELOG.md")}
---
${project.getReleaseDownloadLinksText("${publishS3Dir}/dist", "${getHTTPSTargetUrl(versionText)}")}
"""
                                    writeFile(file: "${publishS3Dir}/${releaseMsgFile}", text: releaseMsg)
                                    sh """
                                        cd ${publishS3Dir}
                                        hub release create -d -f ${releaseMsgFile} \$(find dist/ \\( -name '*.whl' -o -name '*.tar.gz' \\) -exec echo -a {} \\;) "v${versionText}" 
                                    """
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}

//////////////////////
// Helper functions //
//////////////////////
def uploadToS3(artifact, versionText, platformDir) {
    s3upDocker {
        localArtifact = artifact
        artifactId = 'pydatatable'
        version = versionText
        keepPrivate = false
        platform = platformDir
        isRelease = true
    }
}

def getS3TargetUrl(final versionText) {
    return "${S3_URL_STABLE}/datatable-${versionText}/"
}

def getHTTPSTargetUrl(final versionText) {
    return "${HTTPS_URL_STABLE}/datatable-${versionText}/"
}

def getVenvActivationCmd(final platform, final venvName) {
    switch (platform) {
        case 'x86_64_linux':
            return ". /envs/${venvName}/bin/activate"
        case 'x86_64_centos7':
            return ". activate ${venvName}"
        case 'x86_64_macos':
            return ". /Users/jenkins/anaconda/bin/activate ${venvName}"
        case 'ppc64le_linux':
            return ". activate ${venvName}"
    }
}

def isPrJob() {
    return env.CHANGE_BRANCH != null && env.CHANGE_BRANCH != ''
}

def getRelevantTestEnvs() {
    if (isPrJob()) {
        return PR_TEST_ENVIRONMENTS
    }
    return NIGHTLY_TEST_ENVIRONMENTS
}

def isModified(pattern) {
    def fList
    if (isPrJob()) {
        sh "git fetch --no-tags --progress https://github.com/h2oai/datatable +refs/heads/${env.CHANGE_TARGET}:refs/remotes/origin/${env.CHANGE_TARGET}"
        final String mergeBaseSHA = sh(script: "git merge-base HEAD origin/${env.CHANGE_TARGET}", returnStdout: true).trim()
        fList = sh(script: "git diff --name-only ${mergeBaseSHA}", returnStdout: true).trim()
    } else {
        fList = buildInfo.get().getChangedFiles().join('\n')
    }

    def out = ""
    if (!fList.isEmpty()) {
        out = sh(script: "echo '${fList}' | xargs basename -a | egrep -e '${pattern}' | wc -l", returnStdout: true).trim()
    }
    return !(out.isEmpty() || out == "0")
}

def createDockerArgs(linkMap, sourceDir, targetDir) {
    def out = ""
    linkMap.each { key, value ->
        out += "-v ${sourceDir}/${key}:${targetDir}/${value} "
    }
    return out
}

def linkFolders(sourceDir, targetDir) {
    sh """
        mkdir -p ${targetDir}

        mkdir -p ${targetDir}/h2oai-benchmarks
        ln -sf ${sourceDir}/Data ${targetDir}/h2oai-benchmarks

        mkdir -p ${targetDir}/h2o-3
        ln -sf ${sourceDir}/smalldata ${targetDir}/h2o-3
        ln -sf ${sourceDir}/bigdata ${targetDir}/h2o-3
        ln -sf ${sourceDir}/fread ${targetDir}/h2o-3
        find ${targetDir}
    """
}

/**
 * If dockerImage is specified, than execute the body inside container, otherwise execute that on host
 * @param dockerImage docker image to use, or null to run on host
 * @param body body (Closure) to execute
 */
def callInEnv(dockerImage, dockerArgs, body) {
    // Call closure inside dockerImage or directly on host
    if (dockerImage != null) {
        docker.image(dockerImage).inside(dockerArgs) {
            body()
        }
    } else {
        linkFolders(SOURCE_DIR, TARGET_DIR)
        body()
    }
}

def appendLargeTestsSuffixIfRequired(stageName, needsLargerTest) {
    if (needsLargerTest) {
        stageName += ' with large tests'
    }
    return stageName
}

def doPublish() {
    return env.BRANCH_NAME == 'master' || isRelease()
}

def isRelease() {
    return env.BRANCH_NAME.startsWith(RELEASE_BRANCH_PREFIX)
}

